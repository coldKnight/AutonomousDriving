{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import zip_longest\n",
    "import matplotlib.pyplot as plt\n",
    "import csv, argparse\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Convolution2D, Input, Lambda, SpatialDropout2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vatsal/SDN/BitBucketRepos/CarND-Behavioral-Cloning-Project/TEsting\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vatsal/SDN/BitBucketRepos/CarND-Behavioral-Cloning-Project/TEsting/IMG/\n"
     ]
    }
   ],
   "source": [
    "base_dir = os.getcwd()\n",
    "log_path = os.path.join(base_dir, 'driving_log.csv')\n",
    "img_dir = os.path.join(base_dir, 'IMG/')\n",
    "print(img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/vatsal/SDN/BitBucketRepos/CarND-Behavioral-Cloning-Project/TEsting\"\n",
    "LABEL_PATH = \"{}/logs.csv\".format(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.set_image_dim_ordering(\"tf\")\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def import_csv():\n",
    "    \"\"\"\n",
    "    Saving the CSV data in a array\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(LABEL_PATH) as FILE:\n",
    "        reader = csv.reader(FILE)\n",
    "        for i in reader:\n",
    "            data.append(i)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_img(img):\n",
    "    \"\"\"\n",
    "    Load image and crop\n",
    "    \"\"\"\n",
    "    img = \"{}/{}\".format(DATA_PATH, img)\n",
    "    img = plt.imread(img)[60:135, : ]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(data):\n",
    "    \"\"\"\n",
    "    Randomly select batch\n",
    "    \"\"\"\n",
    "    indices = np.random.choice(len(data), BATCH_SIZE)\n",
    "    return data.sample(n=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize_image(data, value):\n",
    "    \"\"\"\n",
    "    Randomize between left, center and right image\n",
    "    And add a shift\n",
    "    \"\"\"\n",
    "    random = np.random.randint(4)\n",
    "    if (random == 0):\n",
    "        path_file = data['left'][value].strip()\n",
    "        shift_ang = .2\n",
    "    if (random == 1 or random == 3):\n",
    "        # Twice as much center images\n",
    "        path_file = data['center'][value].strip()\n",
    "        shift_ang = 0.\n",
    "    if (random == 2):\n",
    "        path_file = data['right'][value].strip()\n",
    "        shift_ang = -.2\n",
    "\n",
    "    return path_file,shift_ang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trans_image(image,steer,trans_range = 100):\n",
    "    \"\"\"\n",
    "    Translation function provided by Vivek Yadav\n",
    "    to augment the steering angles and images randomly\n",
    "    and avoid overfitting\n",
    "    \"\"\"\n",
    "    # Translation\n",
    "    tr_x = trans_range*np.random.uniform()-trans_range/2\n",
    "    steer_ang = steer + tr_x/trans_range*2*.2\n",
    "    tr_y = 0\n",
    "    Trans_M = np.float32([[1,0,tr_x],[0,1,tr_y]])\n",
    "    image_tr = cv2.warpAffine(image,Trans_M,(320,75))\n",
    "    return image_tr,steer_ang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_train(data):\n",
    "    \"\"\"\n",
    "    Train data generator\n",
    "    \"\"\"\n",
    "    obs = 0\n",
    "    while 1:\n",
    "        batch = get_batch(data)\n",
    "        features = np.empty([BATCH_SIZE, 75, 320, 3])\n",
    "        labels = np.empty([BATCH_SIZE, 1])\n",
    "\n",
    "        for i, value in enumerate(batch.index.values):\n",
    "            x, shift = randomize_image(data, value)\n",
    "            x = process_img(x)\n",
    "\n",
    "            x = x.reshape(x.shape[0], x.shape[1], 3)\n",
    "\n",
    "            # Add shift to steer\n",
    "            y = float(data['steer'][value]) + shift\n",
    "\n",
    "            x, y = trans_image(x,y)\n",
    "\n",
    "            random = np.random.randint(1)\n",
    "\n",
    "            # Flip image in 50% of the cases\n",
    "            # Thanks to Vivek Yadav for the idea\n",
    "            if (random == 0):\n",
    "                x = np.fliplr(x)\n",
    "                y = -y\n",
    "\n",
    "            labels[i] = y\n",
    "            features[i] = x\n",
    "\n",
    "        x = np.array(features)\n",
    "        y = np.array(labels)\n",
    "        obs += len(x)\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_valid(data):\n",
    "    \"\"\"\n",
    "    Validation Generator\n",
    "    \"\"\"\n",
    "    while 1:\n",
    "        for i_line in range(len(data)):\n",
    "            data = data.iloc[[i_line]].reset_index()\n",
    "            x = process_img(data['center'][0])\n",
    "            x = x.reshape(1, x.shape[0], x.shape[1], 3)\n",
    "            y = data['steer'][0]\n",
    "            y = np.array([[y]])\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_low_steering(data):\n",
    "    \"\"\"\n",
    "    Remove about 70% of steering values below 0.05\n",
    "    \"\"\"\n",
    "    ind = data[abs(data['steer'])<.03].index.tolist()\n",
    "    rows = []\n",
    "    for i in ind:\n",
    "        random = np.random.randint(10)\n",
    "        if random < 8:\n",
    "            rows.append(i)\n",
    "\n",
    "    data = data.drop(data.index[rows])\n",
    "    print(\"Dropped {} rows with low steering\".format(len(rows)))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nvidia(img):\n",
    "    \"\"\"\n",
    "    Model based on Nvidia paper\n",
    "    http://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    shape = (img[0], img[1], 3)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    def process(img):\n",
    "        import tensorflow as tf\n",
    "        # img = tf.image.rgb_to_grayscale(img)\n",
    "        img = tf.image.resize_images(img, (66, 200))\n",
    "        return img\n",
    "\n",
    "    model.add(Lambda(process, input_shape=shape))\n",
    "\n",
    "    model.add(Lambda(lambda x: x/255.-0.5))\n",
    "    model.add(Convolution2D(24, 5, 5, border_mode=\"same\", subsample=(2,2), activation=\"elu\"))\n",
    "    model.add(SpatialDropout2D(0.2))\n",
    "    model.add(Convolution2D(36, 5, 5, border_mode=\"same\", subsample=(2,2), activation=\"elu\"))\n",
    "    model.add(SpatialDropout2D(0.2))\n",
    "    model.add(Convolution2D(48, 5, 5, border_mode=\"valid\", subsample=(2,2), activation=\"elu\"))\n",
    "    model.add(SpatialDropout2D(0.2))\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode=\"valid\", activation=\"elu\"))\n",
    "    model.add(SpatialDropout2D(0.2))\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode=\"valid\", activation=\"elu\"))\n",
    "    model.add(SpatialDropout2D(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation=\"elu\"))\n",
    "    model.add(Dense(50, activation=\"elu\"))\n",
    "    model.add(Dense(10, activation=\"elu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_9 (Lambda)                (None, 66, 200, 3)    0           lambda_input_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)               (None, 66, 200, 3)    0           lambda_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_21 (Convolution2D) (None, 33, 100, 24)   1824        lambda_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "spatialdropout2d_21 (SpatialDrop (None, 33, 100, 24)   0           convolution2d_21[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_22 (Convolution2D) (None, 17, 50, 36)    21636       spatialdropout2d_21[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "spatialdropout2d_22 (SpatialDrop (None, 17, 50, 36)    0           convolution2d_22[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_23 (Convolution2D) (None, 7, 23, 48)     43248       spatialdropout2d_22[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "spatialdropout2d_23 (SpatialDrop (None, 7, 23, 48)     0           convolution2d_23[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_24 (Convolution2D) (None, 5, 21, 64)     27712       spatialdropout2d_23[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "spatialdropout2d_24 (SpatialDrop (None, 5, 21, 64)     0           convolution2d_24[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_25 (Convolution2D) (None, 3, 19, 64)     36928       spatialdropout2d_24[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "spatialdropout2d_25 (SpatialDrop (None, 3, 19, 64)     0           convolution2d_25[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (None, 3648)          0           spatialdropout2d_25[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 3648)          0           flatten_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_17 (Dense)                 (None, 100)           364900      dropout_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_18 (Dense)                 (None, 50)            5050        dense_17[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_19 (Dense)                 (None, 10)            510         dense_18[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 10)            0           dense_19[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_20 (Dense)                 (None, 1)             11          dropout_10[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 501,819\n",
      "Trainable params: 501,819\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Dropped 336 rows with low steering\n",
      "Epoch 1/5\n",
      "50/64 [======================>.......] - ETA: 0s - loss: 0.3425"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vatsal/anaconda3/envs/behav-test/lib/python3.5/site-packages/keras/engine/training.py:1537: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/64 [==============================================] - 4s - loss: 0.3937 - val_loss: 0.3117\n",
      "Epoch 2/5\n",
      "100/64 [==============================================] - 2s - loss: 0.4826 - val_loss: 0.0527\n",
      "Epoch 3/5\n",
      "100/64 [==============================================] - 3s - loss: 0.3974 - val_loss: 0.0766\n",
      "Epoch 4/5\n",
      "100/64 [==============================================] - 2s - loss: 0.4448 - val_loss: 0.0907\n",
      "Epoch 5/5\n",
      "100/64 [==============================================] - 2s - loss: 0.3653 - val_loss: 0.1038\n",
      "It's saved now\n"
     ]
    }
   ],
   "source": [
    "# 0 = center\n",
    "# 1 = left\n",
    "# 2 = right\n",
    "# 3 = steering angle\n",
    "\n",
    "for i in range(1):\n",
    "    # Train the network x times\n",
    "    # Load data\n",
    "    data = pd.read_csv(LABEL_PATH, index_col=False)\n",
    "    data.columns = ['center', 'left', 'right', 'steer']\n",
    "\n",
    "    img = process_img(data['center'][100].strip())\n",
    "\n",
    "    model = nvidia(img.shape)\n",
    "    model.summary()\n",
    "    model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "\n",
    "    # Shuffle data\n",
    "    data_shuffle = data.reindex(np.random.permutation(data.index))\n",
    "\n",
    "    # Split data on a multiple of BATCH SIZE\n",
    "    split = (int(len(data_shuffle) * 0.9) // BATCH_SIZE) * BATCH_SIZE\n",
    "    train_data = data[:split]\n",
    "\n",
    "    train_data = remove_low_steering(train_data)\n",
    "\n",
    "    val_data = data[split:]\n",
    "    new_val = (len(val_data) // BATCH_SIZE) * BATCH_SIZE\n",
    "    val_data = val_data[:new_val]\n",
    "\n",
    "    samples_per_epoch = len(train_data) - BATCH_SIZE\n",
    "\n",
    "    values = model.fit_generator(generate_train(train_data), \n",
    "                                 samples_per_epoch=samples_per_epoch, \n",
    "                                 nb_epoch=EPOCHS, \n",
    "                                 validation_data=generate_train(val_data), \n",
    "                                 nb_val_samples=len(val_data))\n",
    "\n",
    "    model_rep = model.to_json()\n",
    "\n",
    "    # Save data\n",
    "    with open('model.json', 'w') as f:\n",
    "        json.dump(model_rep, f)\n",
    "\n",
    "        model.save_weights('./model.h5')\n",
    "\n",
    "        print(\"It's saved now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:behav-test]",
   "language": "python",
   "name": "conda-env-behav-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
